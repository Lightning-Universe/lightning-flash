{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/PyTorchLightning/lightning-flash/blob/master/flash_notebooks/text_classification.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we'll go over the basics of lightning Flash by finetunig a TextClassifier on [IMDB Dataset](https://www.imdb.com/interfaces/).\n",
    "\n",
    "# Finetuning\n",
    "\n",
    "Finetuning consists of four steps:\n",
    " \n",
    " - 1. Train a source neural network model on a source dataset. For text classication, it is traditionally  a transformer model such as BERT [Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805) trained on wikipedia.\n",
    "As those model are costly to train, [Transformers](https://github.com/huggingface/transformers) or [FairSeq](https://github.com/pytorch/fairseq) libraries provides popular pre-trained model architectures for NLP. In this notebook, we will be using [tiny-bert](https://huggingface.co/prajjwal1/bert-tiny).\n",
    "\n",
    " \n",
    " - 2. Create a new neural network the target model. Its architecture replicates all model designs and their parameters on the source model, expect the latest layer which is removed. This model without its latest layers is traditionally called a backbone\n",
    " \n",
    "\n",
    "- 3. Add new layers after the backbone where the latest output size is the number of target dataset categories. Those new layers, traditionally called head, will be randomly initialized while backbone will conserve its pre-trained weights from ImageNet.\n",
    " \n",
    "\n",
    "- 4. Train the target model on a target dataset, such as Hymenoptera Dataset with ants and bees. However, freezing some layers at training start such as the backbone tends to be more stable. In Flash, it can easily be done with `trainer.finetune(..., strategy=\"freeze\")`. It is also common to `freeze/unfreeze` the backbone. In `Flash`, it can be done with `trainer.finetune(..., strategy=\"freeze_unfreeze\")`. If a one wants more control on the unfreeze flow, Flash supports `trainer.finetune(..., strategy=MyFinetuningStrategy())` where `MyFinetuningStrategy` is subclassing `pytorch_lightning.callbacks.BaseFinetuning`.\n",
    "\n",
    "---\n",
    "  - Give us a â­ [on Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n",
    "  - Check out [Flash documentation](https://lightning-flash.readthedocs.io/en/latest/)\n",
    "  - Check out [Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "  - Join us [on Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup  \n",
    "Lightning Flash is easy to install. Simply ```pip install lightning-flash```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\n",
    "! pip install 'git+https://github.com/PyTorchLightning/lightning-flash.git#egg=lightning-flash[text]'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "input_ids = torch.tensor([[1, 2, 3]])\n",
    "attention_mask = torch.tensor([[0, 0, 0]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from functools import partial, wraps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def f(a, b, c):\n",
    "    print(a, b, c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "a, b, c = \"a, b, c\".split(\",\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "g = partial(f, a)\n",
    "g(b, c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a  b  c\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def print_provider_info(name, providers, func):\n",
    "    message = f\"Using '{name}' provided by {', '.join(str(provider) for provider in providers)}.\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(message)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "fn = partial(f, a)\n",
    "fn = print_provider_info(a, [\"mimmo\"], fn)\n",
    "fn(b, c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using 'a' provided by mimmo.\n",
      "a  b  c\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a  b  c\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "?AutoModel.from_config"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Instantiates one of the base model classes of the library from a configuration.\n",
      "\n",
      "Note:\n",
      "    Loading a model from its configuration file does **not** load the model weights. It only affects the\n",
      "    model's configuration. Use :meth:`~transformers.AutoModel.from_pretrained` to load the model\n",
      "    weights.\n",
      "\n",
      "Args:\n",
      "    config (:class:`~transformers.PretrainedConfig`):\n",
      "        The model class to instantiate is selected based on the configuration class:\n",
      "\n",
      "        - :class:`~transformers.AlbertConfig` configuration class: :class:`~transformers.AlbertModel` (ALBERT model)\n",
      "        - :class:`~transformers.BartConfig` configuration class: :class:`~transformers.BartModel` (BART model)\n",
      "        - :class:`~transformers.BeitConfig` configuration class: :class:`~transformers.BeitModel` (BeiT model)\n",
      "        - :class:`~transformers.BertConfig` configuration class: :class:`~transformers.BertModel` (BERT model)\n",
      "        - :class:`~transformers.BertGenerationConfig` configuration class: :class:`~transformers.BertGenerationEncoder` (Bert Generation model)\n",
      "        - :class:`~transformers.BigBirdConfig` configuration class: :class:`~transformers.BigBirdModel` (BigBird model)\n",
      "        - :class:`~transformers.BigBirdPegasusConfig` configuration class: :class:`~transformers.BigBirdPegasusModel` (BigBirdPegasus model)\n",
      "        - :class:`~transformers.BlenderbotConfig` configuration class: :class:`~transformers.BlenderbotModel` (Blenderbot model)\n",
      "        - :class:`~transformers.BlenderbotSmallConfig` configuration class: :class:`~transformers.BlenderbotSmallModel` (BlenderbotSmall model)\n",
      "        - :class:`~transformers.CLIPConfig` configuration class: :class:`~transformers.CLIPModel` (CLIP model)\n",
      "        - :class:`~transformers.CTRLConfig` configuration class: :class:`~transformers.CTRLModel` (CTRL model)\n",
      "        - :class:`~transformers.CamembertConfig` configuration class: :class:`~transformers.CamembertModel` (CamemBERT model)\n",
      "        - :class:`~transformers.CanineConfig` configuration class: :class:`~transformers.CanineModel` (Canine model)\n",
      "        - :class:`~transformers.ConvBertConfig` configuration class: :class:`~transformers.ConvBertModel` (ConvBERT model)\n",
      "        - :class:`~transformers.DPRConfig` configuration class: :class:`~transformers.DPRQuestionEncoder` (DPR model)\n",
      "        - :class:`~transformers.DebertaConfig` configuration class: :class:`~transformers.DebertaModel` (DeBERTa model)\n",
      "        - :class:`~transformers.DebertaV2Config` configuration class: :class:`~transformers.DebertaV2Model` (DeBERTa-v2 model)\n",
      "        - :class:`~transformers.DeiTConfig` configuration class: :class:`~transformers.DeiTModel` (DeiT model)\n",
      "        - :class:`~transformers.DetrConfig` configuration class: :class:`~transformers.DetrModel` (DETR model)\n",
      "        - :class:`~transformers.DistilBertConfig` configuration class: :class:`~transformers.DistilBertModel` (DistilBERT model)\n",
      "        - :class:`~transformers.ElectraConfig` configuration class: :class:`~transformers.ElectraModel` (ELECTRA model)\n",
      "        - :class:`~transformers.FSMTConfig` configuration class: :class:`~transformers.FSMTModel` (FairSeq Machine-Translation model)\n",
      "        - :class:`~transformers.FlaubertConfig` configuration class: :class:`~transformers.FlaubertModel` (FlauBERT model)\n",
      "        - :class:`~transformers.FunnelConfig` configuration class: :class:`~transformers.FunnelModel` or :class:`~transformers.FunnelBaseModel` (Funnel Transformer model)\n",
      "        - :class:`~transformers.GPT2Config` configuration class: :class:`~transformers.GPT2Model` (OpenAI GPT-2 model)\n",
      "        - :class:`~transformers.GPTNeoConfig` configuration class: :class:`~transformers.GPTNeoModel` (GPT Neo model)\n",
      "        - :class:`~transformers.HubertConfig` configuration class: :class:`~transformers.HubertModel` (Hubert model)\n",
      "        - :class:`~transformers.IBertConfig` configuration class: :class:`~transformers.IBertModel` (I-BERT model)\n",
      "        - :class:`~transformers.LEDConfig` configuration class: :class:`~transformers.LEDModel` (LED model)\n",
      "        - :class:`~transformers.LayoutLMConfig` configuration class: :class:`~transformers.LayoutLMModel` (LayoutLM model)\n",
      "        - :class:`~transformers.LayoutLMv2Config` configuration class: :class:`~transformers.LayoutLMv2Model` (LayoutLMv2 model)\n",
      "        - :class:`~transformers.LongformerConfig` configuration class: :class:`~transformers.LongformerModel` (Longformer model)\n",
      "        - :class:`~transformers.LukeConfig` configuration class: :class:`~transformers.LukeModel` (LUKE model)\n",
      "        - :class:`~transformers.LxmertConfig` configuration class: :class:`~transformers.LxmertModel` (LXMERT model)\n",
      "        - :class:`~transformers.M2M100Config` configuration class: :class:`~transformers.M2M100Model` (M2M100 model)\n",
      "        - :class:`~transformers.MBartConfig` configuration class: :class:`~transformers.MBartModel` (mBART model)\n",
      "        - :class:`~transformers.MPNetConfig` configuration class: :class:`~transformers.MPNetModel` (MPNet model)\n",
      "        - :class:`~transformers.MT5Config` configuration class: :class:`~transformers.MT5Model` (mT5 model)\n",
      "        - :class:`~transformers.MarianConfig` configuration class: :class:`~transformers.MarianModel` (Marian model)\n",
      "        - :class:`~transformers.MegatronBertConfig` configuration class: :class:`~transformers.MegatronBertModel` (MegatronBert model)\n",
      "        - :class:`~transformers.MobileBertConfig` configuration class: :class:`~transformers.MobileBertModel` (MobileBERT model)\n",
      "        - :class:`~transformers.OpenAIGPTConfig` configuration class: :class:`~transformers.OpenAIGPTModel` (OpenAI GPT model)\n",
      "        - :class:`~transformers.PegasusConfig` configuration class: :class:`~transformers.PegasusModel` (Pegasus model)\n",
      "        - :class:`~transformers.ProphetNetConfig` configuration class: :class:`~transformers.ProphetNetModel` (ProphetNet model)\n",
      "        - :class:`~transformers.ReformerConfig` configuration class: :class:`~transformers.ReformerModel` (Reformer model)\n",
      "        - :class:`~transformers.RemBertConfig` configuration class: :class:`~transformers.RemBertModel` (RemBERT model)\n",
      "        - :class:`~transformers.RetriBertConfig` configuration class: :class:`~transformers.RetriBertModel` (RetriBERT model)\n",
      "        - :class:`~transformers.RoFormerConfig` configuration class: :class:`~transformers.RoFormerModel` (RoFormer model)\n",
      "        - :class:`~transformers.RobertaConfig` configuration class: :class:`~transformers.RobertaModel` (RoBERTa model)\n",
      "        - :class:`~transformers.Speech2TextConfig` configuration class: :class:`~transformers.Speech2TextModel` (Speech2Text model)\n",
      "        - :class:`~transformers.SplinterConfig` configuration class: :class:`~transformers.SplinterModel` (Splinter model)\n",
      "        - :class:`~transformers.SqueezeBertConfig` configuration class: :class:`~transformers.SqueezeBertModel` (SqueezeBERT model)\n",
      "        - :class:`~transformers.T5Config` configuration class: :class:`~transformers.T5Model` (T5 model)\n",
      "        - :class:`~transformers.TapasConfig` configuration class: :class:`~transformers.TapasModel` (TAPAS model)\n",
      "        - :class:`~transformers.TransfoXLConfig` configuration class: :class:`~transformers.TransfoXLModel` (Transformer-XL model)\n",
      "        - :class:`~transformers.ViTConfig` configuration class: :class:`~transformers.ViTModel` (ViT model)\n",
      "        - :class:`~transformers.VisualBertConfig` configuration class: :class:`~transformers.VisualBertModel` (VisualBert model)\n",
      "        - :class:`~transformers.Wav2Vec2Config` configuration class: :class:`~transformers.Wav2Vec2Model` (Wav2Vec2 model)\n",
      "        - :class:`~transformers.XLMConfig` configuration class: :class:`~transformers.XLMModel` (XLM model)\n",
      "        - :class:`~transformers.XLMProphetNetConfig` configuration class: :class:`~transformers.XLMProphetNetModel` (XLMProphetNet model)\n",
      "        - :class:`~transformers.XLMRobertaConfig` configuration class: :class:`~transformers.XLMRobertaModel` (XLM-RoBERTa model)\n",
      "        - :class:`~transformers.XLNetConfig` configuration class: :class:`~transformers.XLNetModel` (XLNet model)\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> from transformers import AutoConfig, AutoModel\n",
      "    >>> # Download configuration from huggingface.co and cache.\n",
      "    >>> config = AutoConfig.from_pretrained('bert-base-cased')\n",
      "    >>> model = AutoModel.from_config(config)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/flash-dev/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from flash.core.registry import FlashRegistry\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "register = FlashRegistry(\"ciao\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "register(fn=lambda x: x ** 2, name=\"squares\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "register.get(\"squares\")(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.0980,  1.6983, -2.0286,  1.8005, -0.1955,  1.5695,  0.6801,  0.0172,\n",
       "          0.0210,  0.2806, -2.4556, -0.5767, -0.0667,  0.1077,  1.6762,  0.1255,\n",
       "         -0.7593,  1.3331, -0.6025,  0.9293,  0.2805,  0.6155,  0.5316,  0.8547,\n",
       "         -0.4913,  1.1260, -0.5975,  0.8861,  2.0638, -0.3643, -0.4109,  0.1442,\n",
       "          0.0918,  0.7337, -0.7533,  0.0958, -0.3635, -0.1696, -1.1720, -1.2479,\n",
       "          0.4800, -0.0416, -0.9969, -0.4413,  0.1434, -0.2172, -0.4719,  0.3363,\n",
       "         -1.8119, -2.9664, -0.2746,  0.3044,  0.5981,  0.4752,  0.0835, -0.2949,\n",
       "         -0.8395, -0.1491,  0.0084,  0.0396, -0.0845, -0.4880,  0.5800,  0.1660,\n",
       "          1.0523,  0.7101, -1.1045, -0.5234,  2.1414,  0.1768, -1.8038, -2.1305,\n",
       "         -1.3730,  1.2224, -2.2038,  0.2272, -0.1295,  1.5207,  0.3886,  0.6455,\n",
       "          0.1510,  0.4063, -1.6927, -0.9324, -0.4468,  0.1298,  1.6526, -2.0272,\n",
       "         -0.1826, -0.4826, -0.0427, -1.5033,  0.3871, -0.4460,  0.7717,  0.9557,\n",
       "          0.1360, -1.2174, -0.8140, -1.2283,  0.1068,  0.1684,  0.6546, -0.5011,\n",
       "         -1.7349,  0.3945,  1.7975, -1.1939,  0.3290, -0.3868,  0.8752,  2.3213,\n",
       "          1.5104,  0.3004,  0.2664, -1.5909,  0.7768,  0.0153,  0.8900, -1.1348,\n",
       "          0.1380,  0.4857, -0.0318, -0.2605,  1.2323,  1.2113,  0.1118,  0.2161]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.6779,  0.2137, -3.0522, -3.6507,  0.2144, -1.2370, -0.5905, -0.2969,\n",
       "         -0.9668,  1.1663,  1.0880,  0.4692, -0.6553, -0.3710,  0.0250, -0.2260,\n",
       "         -0.3706,  0.8032, -1.4529,  0.8649,  0.4158, -0.4853,  1.0500,  1.7995,\n",
       "          1.2632,  0.1047,  0.2858, -0.0896,  0.2542, -0.4467, -0.8571, -0.6128,\n",
       "         -1.5168, -0.6560, -0.2723, -0.8820,  0.6165, -0.7374, -1.6829, -1.1235,\n",
       "          0.8140, -0.6164,  3.7400, -1.7932,  0.7968, -0.4430, -0.7920, -1.2361,\n",
       "          0.5063, -0.5180, -0.0926,  0.3824,  0.4915, -0.9325,  0.1472, -1.0297,\n",
       "          0.7198,  0.5642, -0.4940,  1.5990,  0.5750, -0.1140, -0.4705, -0.9576,\n",
       "         -0.9040,  1.2670,  0.5872, -0.2413,  0.8461, -0.4272,  0.3727, -1.3484,\n",
       "         -1.0625, -0.5191, -1.8573,  0.3746,  1.2833,  0.9423,  3.2822,  0.6382,\n",
       "          1.1604,  0.3568, -0.1511, -0.6394,  0.2059,  0.2658, -0.5631, -0.3437,\n",
       "          2.2515, -1.0959,  0.8620,  0.9133, -0.7004,  0.6072,  1.4474,  1.3105,\n",
       "         -0.0406, -0.0971, -0.2112,  0.5907, -0.0406, -0.4666,  0.5843, -1.0075,\n",
       "          0.9176, -0.4503, -0.7579, -0.1501, -0.2404, -1.5607, -0.0821,  1.8354,\n",
       "          0.5239, -0.0667, -0.8510,  0.3150,  1.0031, -0.9414,  1.0766, -1.2302,\n",
       "         -0.8493,  0.8842,  0.7735, -1.5269, -0.5857, -2.3115, -0.3179,  0.7460]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import flash\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.text import TextClassificationData"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (transfomers.py, line 36)",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/49796/miniconda3/envs/flash-dev/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3441\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/var/folders/1l/rbqk1ct16r1cbwrzhlkm8ltm6g1fy8/T/ipykernel_21383/1628330777.py\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from flash.text import TextClassificationData\n",
      "  File \u001b[1;32m\"/Users/49796/Documents/GitHub/lightning-flash/flash/text/__init__.py\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from flash.text.classification import TextClassificationData, TextClassifier  # noqa: F401\n",
      "  File \u001b[1;32m\"/Users/49796/Documents/GitHub/lightning-flash/flash/text/classification/__init__.py\"\u001b[0m, line \u001b[1;32m15\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from flash.text.classification.model import TextClassifier  # noqa: F401\n",
      "  File \u001b[1;32m\"/Users/49796/Documents/GitHub/lightning-flash/flash/text/classification/model.py\"\u001b[0m, line \u001b[1;32m27\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from flash.text.classification.backbones import TEXT_CLASSIFIER_BACKBONES\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/49796/Documents/GitHub/lightning-flash/flash/text/classification/backbones/__init__.py\"\u001b[0;36m, line \u001b[0;32m27\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from flash.text.classification.backbones.transfomers import _transfomer\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/49796/Documents/GitHub/lightning-flash/flash/text/classification/backbones/transfomers.py\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    name=\"\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  1. Download the data\n",
    "The data are downloaded from a URL, and save in a 'data' directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/imdb.zip\", 'data/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>2. Load the data</h2>\n",
    "\n",
    "Flash Tasks have built-in DataModules that you can use to organize your data. Pass in a train, validation and test folders and Flash will take care of the rest.\n",
    "Creates a TextClassificationData object from csv file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "datamodule = TextClassificationData.from_csv(\n",
    "    train_file=\"data/imdb/train.csv\",\n",
    "    val_file=\"data/imdb/valid.csv\",\n",
    "    test_file=\"data/imdb/test.csv\",\n",
    "    input_fields=\"review\",\n",
    "    target_fields=\"sentiment\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TextClassificationData' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1l/rbqk1ct16r1cbwrzhlkm8ltm6g1fy8/T/ipykernel_21383/898055592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m datamodule = TextClassificationData.from_csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/imdb/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mval_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/imdb/valid.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/imdb/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextClassificationData' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  3. Build the model\n",
    "\n",
    "Create the TextClassifier task. By default, the TextClassifier task uses a [tiny-bert](https://huggingface.co/prajjwal1/bert-tiny) backbone to train or finetune your model demo. You could use any models from [transformers - Text Classification](https://huggingface.co/models?filter=text-classification,pytorch)\n",
    "\n",
    "Backbone can easily be changed with such as `TextClassifier(backbone='bert-tiny-mnli')`"
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model = TextClassifier(num_classes=datamodule.num_classes, backbone=\"bert-tiny\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'TextClassifier' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1l/rbqk1ct16r1cbwrzhlkm8ltm6g1fy8/T/ipykernel_21383/1870168566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-tiny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TextClassifier' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  4. Create the trainer. Run once on data"
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = flash.Trainer(max_epochs=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  5. Fine-tune the model\n",
    "\n",
    "The backbone won't be freezed and the entire model will be finetuned on the imdb dataset "
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  6. Test model"
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.test(model, datamodule=datamodule)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  7. Save it!"
   ],
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.save_checkpoint(\"text_classification_model.pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load the model from a checkpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = TextClassifier.load_from_checkpoint(\"https://flash-weights.s3.amazonaws.com/text_classification_model.pt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2a. Classify a few sentences! How was the movie?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model.predict([\n",
    "    \"Turgid dialogue, feeble characterization - Harvey Keitel a judge?.\",\n",
    "    \"The worst movie in the history of cinema.\",\n",
    "    \"I come from Bulgaria where it 's almost impossible to have a tornado.\"\n",
    "    \"Very, very afraid\"\n",
    "    \"This guy has done a great job with this movie!\",\n",
    "])\n",
    "print(predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b. Or generate predictions from a sheet file!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "datamodule = TextClassificationData.from_csv(\n",
    "    predict_file=\"data/imdb/predict.csv\",\n",
    "    input_fields=\"review\",\n",
    ")\n",
    "predictions = flash.Trainer().predict(model, datamodule=datamodule)\n",
    "print(predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<code style=\"color:#792ee5;\">\n",
    "    <h1> <strong> Congratulations - Time to Join the Community! </strong>  </h1>\n",
    "</code>\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed it and would like to join the Lightning movement, you can do so in the following ways!\n",
    "\n",
    "### Help us build Flash by adding support for new data-types and new tasks.\n",
    "Flash aims at becoming the first task hub, so anyone can get started to great amazing application using deep learning. \n",
    "If you are interested, please open a PR with your contributions !!! \n",
    "\n",
    "\n",
    "### Star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "* Please, star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "\n",
    "### Join our [Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-pw5v393p-qRaDgEk24~EjiZNBpSQFgQ)!\n",
    "The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in `#general` channel\n",
    "\n",
    "### Interested by SOTA AI models ! Check out [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "Bolts has a collection of state-of-the-art models, all implemented in [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) and can be easily integrated within your own projects.\n",
    "\n",
    "* Please, star [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "\n",
    "### Contributions !\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) or [Bolt](https://github.com/PyTorchLightning/lightning-bolts) GitHub Issues page and filter for \"good first issue\". \n",
    "\n",
    "* [Lightning good first issue](https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* [Bolt good first issue](https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* You can also contribute your own notebooks with useful examples !\n",
    "\n",
    "### Great thanks from the entire Pytorch Lightning Team for your interest !\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PyTorchLightning/lightning-flash/18c591747e40a0ad862d4f82943d209b8cc25358/docs/source/_static/images/logo.svg\" width=\"800\" height=\"200\" />"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('flash-dev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "interpreter": {
   "hash": "6c2e38ed56ebe2833c80d1b40a6bb0719d1f89bc272c0d98043136690c67fb4f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}