{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/PyTorchLightning/lightning-flash/blob/master/flash_notebooks/custom_task_tutorial\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Creating a Custom Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will go over the process of creating a custom task, along with a custom data module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install git+https://github.com/PyTorchLightning/pytorch-flash.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flash\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "Here we create a basic linear regression task by subclassing `flash.Task`. For the majority of tasks, you will likely only need to override the `__init__` and `forward` methods of task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(flash.Task):\n",
    "    def __init__(self, num_inputs, learning_rate=0.001, metrics=None):\n",
    "        # what kind of model do we want?\n",
    "        model = nn.Linear(num_inputs, 1)\n",
    "\n",
    "        # what loss function do we want?\n",
    "        loss_fn = torch.nn.functional.mse_loss\n",
    "        \n",
    "        # what optimizer to do we want?\n",
    "        optimizer = torch.optim.SGD\n",
    "        \n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics,\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # we don't actually need to override this method for this example\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is the training step?\n",
    "\n",
    "Most models can be trained simply by passing the output of `forward` to the supplied `loss_fn`, and then passing the resulting loss to the supplied `optimizer`. If you need a more custom configuration, you can override `step` (which is called for training, validation, and testing) or override `training_step`, `validation_step`, and `test_step` individually. These methods behave identically to PyTorch Lightning's [methods](https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#methods).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For a task you will likely need a specific way of loading data. For this example, lets say we want a `flash.DataModule` to be used explicitly for the prediction of diabetes disease progression. We can create this `DataModule` below, wrapping the scikit-learn [Diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesPipeline(flash.data.process.Postprocess):\n",
    "    def per_sample_transform(self, samples):\n",
    "        return [f\"disease progression: {float(s):.2f}\" for s in samples]\n",
    "\n",
    "class DiabetesData(flash.DataModule):\n",
    "    \n",
    "    postprocess_cls = DiabetesPipeline\n",
    "    \n",
    "    def __init__(self, batch_size=64, num_workers=0):\n",
    "        x, y = datasets.load_diabetes(return_X_y=True)\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float().unsqueeze(1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=0)\n",
    "\n",
    "        train_ds = TensorDataset(x_train, y_train)\n",
    "        test_ds = TensorDataset(x_test, y_test)\n",
    "        \n",
    "        super().__init__(\n",
    "            train_ds=train_ds,\n",
    "            test_ds=test_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        self.num_inputs = x.shape[1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice we added a `DataPipeline`, which will be used when we call `.predict()` on our model. In this case we want to nicely format our ouput from the model with the string `\"disease progression\"`, but you could do any sort of post processing you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like any Flash Task, we can fit our model using the `flash.Trainer` by supplying the task itself, and the associated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DiabetesData()\n",
    "model = LinearRegression(num_inputs=data.num_inputs)\n",
    "\n",
    "trainer = flash.Trainer(max_epochs=10, progress_bar_refresh_rate=20)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a trained model we can now perform inference. Here we will use a few examples from the test set of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-21fe9b174a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m predict_data = torch.tensor([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m \u001b[0;36m0.0199\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0507\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.1048\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0701\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0360\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0267\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0026\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0037\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0403\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.0128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0446\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0606\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0529\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0480\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0294\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0176\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0343\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0702\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0072\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m[\u001b[0m \u001b[0;36m0.0381\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0507\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0089\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0425\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0428\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0210\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0397\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0026\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0181\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0072\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.0128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0446\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0235\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0167\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0.0046\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0176\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0026\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0385\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "predict_data = torch.tensor([\n",
    "    [ 0.0199,  0.0507,  0.1048,  0.0701, -0.0360, -0.0267, -0.0250, -0.0026, 0.0037,  0.0403],\n",
    "    [-0.0128, -0.0446,  0.0606,  0.0529,  0.0480,  0.0294, -0.0176,  0.0343, 0.0702,  0.0072],\n",
    "    [ 0.0381,  0.0507,  0.0089,  0.0425, -0.0428, -0.0210, -0.0397, -0.0026, -0.0181,  0.0072],\n",
    "    [-0.0128, -0.0446, -0.0235, -0.0401, -0.0167,  0.0046, -0.0176, -0.0026, -0.0385, -0.0384],\n",
    "    [-0.0237, -0.0446,  0.0455,  0.0907, -0.0181, -0.0354,  0.0707, -0.0395, -0.0345, -0.0094]])\n",
    "\n",
    "model.predict(predict_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of our custom data pipeline's `after_uncollate` method, we will get a nicely formatted output like the following:\n",
    "```\n",
    "[['disease progression: 14.84'],\n",
    " ['disease progression: 14.86'],\n",
    " ['disease progression: 14.78'],\n",
    " ['disease progression: 14.73'],\n",
    " ['disease progression: 14.71']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"color:#792ee5;\">\n",
    "    <h1> <strong> Congratulations - Time to Join the Community! </strong>  </h1>\n",
    "</code>\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed it and would like to join the Lightning movement, you can do so in the following ways!\n",
    "\n",
    "### Help us build Flash by adding support for new data-types and new tasks.\n",
    "Flash aims at becoming the first task hub, so anyone can get started to great amazing application using deep learning. \n",
    "If you are interested, please open a PR with your contributions !!! \n",
    "\n",
    "\n",
    "### Star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "* Please, star [Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "\n",
    "### Join our [Slack](https://join.slack.com/t/pytorch-lightning/shared_invite/zt-f6bl2l0l-JYMK3tbAgAmGRrlNr00f1A)!\n",
    "The best way to keep up to date on the latest advancements is to join our community! Make sure to introduce yourself and share your interests in `#general` channel\n",
    "\n",
    "### Interested by SOTA AI models ! Check out [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "Bolts has a collection of state-of-the-art models, all implemented in [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) and can be easily integrated within your own projects.\n",
    "\n",
    "* Please, star [Bolt](https://github.com/PyTorchLightning/lightning-bolts)\n",
    "\n",
    "### Contributions !\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to [Lightning](https://github.com/PyTorchLightning/pytorch-lightning) or [Bolt](https://github.com/PyTorchLightning/lightning-bolts) GitHub Issues page and filter for \"good first issue\". \n",
    "\n",
    "* [Lightning good first issue](https://github.com/PyTorchLightning/pytorch-lightning/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* [Bolt good first issue](https://github.com/PyTorchLightning/lightning-bolts/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "* You can also contribute your own notebooks with useful examples !\n",
    "\n",
    "### Great thanks from the entire Pytorch Lightning Team for your interest !\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PyTorchLightning/lightning-flash/18c591747e40a0ad862d4f82943d209b8cc25358/docs/source/_static/images/logo.svg\" width=\"800\" height=\"200\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
